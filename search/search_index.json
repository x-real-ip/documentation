{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to my Homelab documentation","text":"<p>This documentation is where I track my infrastructure setup, how-to's, cheatsheet, experiments and automation projects. My lab runs primarily on a Proxmox cluster with TrueNAS SCALE for storage and Kubernetes (k3s) for running services. I use Ansible to automate deployments and manage various operational tasks. Additionally, I follow a GitOps approach for Kubernetes using a dedicated repository with ArgoCD to declaratively manage my services and configurations.</p> <p>The goals of this documentation are twofold:</p> <ul> <li>To serve as a reference for my own environment and workflows.</li> <li>To share ideas, configurations, and lessons learned that may help others building or maintaining their own homelab.</li> </ul> <p>Here, you\u2019ll find notes, guides, and code snippets covering virtualization, storage, and automation, all tested and documented from a real-world setup.</p>"},{"location":"#serve-docs-locally","title":"Serve docs locally","text":"<pre><code>podman run --rm -it -v ${PWD}:/docs -p 8000:8000 docker.io/squidfunk/mkdocs-material serve -a 0.0.0.0:8000\n</code></pre>"},{"location":"backup/","title":"Backup","text":""},{"location":"backup/#3-2-1-backup-strategy","title":"3-2-1 Backup Strategy","text":"<p>3 Copies, 2 media volumes (truenas-a and truenas-b), 1 off-site (truenas-c)</p>"},{"location":"backup/#off-site","title":"off-site","text":"<p>Off-site backup is a replication task from the truenas-master to truenas-c, it is connected via VPN using wireguard.</p> <p>Configure wireguard connection after boot on truenas-c</p> <ol> <li>Create a <code>wg0.conf</code> in <code>/data/wg0.conf</code> only this directory is persistent.</li> <li> <p>Add the following content (see bitwarden for full config including keys)     <pre><code> [Interface]\n PrivateKey = **removed**\n Address = 10.44.44.3/32\n DNS = 1.1.1.1\n\n [Peer]\n PublicKey = **removed**\n Endpoint = **removed**:51820\n AllowedIPs = 10.44.44.3/32,10.0.10.0/24,10.0.100.0/24,10.42.0.0/16\n PersistentKeepalive = 25\n</code></pre></p> </li> <li> <p>Go to <code>/ui/system/initshutdown</code> and configure the init command as follow:</p> <ul> <li>Type: Command</li> <li>Description: wireguard up at boot</li> <li>When: Post Init</li> <li>Command/Script: <code>/usr/bin/wg-quick up /data/wg0.conf</code></li> <li>Enabled: Yes</li> </ul> </li> </ol>"},{"location":"cheatsheet/","title":"Cheatsheet","text":""},{"location":"cheatsheet/#truenas","title":"TrueNAS","text":""},{"location":"cheatsheet/#rename-volume","title":"Rename volume","text":"<pre><code>zfs rename r01_1tb/k8s/{current zvol name} r01_1tb/k8s/{new zvol name}\n</code></pre>"},{"location":"cheatsheet/#resize-vm-disk","title":"Resize VM disk","text":"<pre><code>sudo dnf install cloud-utils-growpart\n</code></pre> <pre><code>sudo growpart /dev/sda 2\n</code></pre> <pre><code>sudo pvresize /dev/sda2\n</code></pre> <pre><code>sudo lvextend -l +100%FREE /dev/mapper/rl-root\n</code></pre> <pre><code>sudo xfs_growfs /\n</code></pre>"},{"location":"cheatsheet/#repair-iscsi-share","title":"Repair iSCSI share","text":""},{"location":"cheatsheet/#script","title":"Script","text":"<ol> <li>Run the following script to start a interactive script that repair the iSCSI    share</li> </ol> <pre><code>curl -sSL https://raw.githubusercontent.com/x-real-ip/infrastructure/refs/heads/main/scripts/repair_iscsi_volume.sh | bash\n</code></pre>"},{"location":"cheatsheet/#manually","title":"Manually","text":"<ol> <li>Make sure that the container that uses the volume has stopped.</li> <li> <p>SSH into one of the nodes in the cluster and start discovery</p> <pre><code>sudo iscsiadm -m discovery -t st -p truenas-master.lan.stamx.nl &amp;&amp; \\\nread -p \"Enter the disk name: \" DISKNAME &amp;&amp; \\\nexport DISKNAME\n</code></pre> </li> <li> <p>Login to target</p> <pre><code>sudo iscsiadm --mode node --targetname iqn.2005-10.org.freenas.ctl:${DISKNAME} --portal truenas-master.lan.stamx.nl --login &amp;&amp; \\\nsleep 5 &amp;&amp; \\\nlsblk &amp;&amp; \\\nread -p \"Enter the device ('sda' for example): \" DEVICENAME &amp;&amp; \\\nexport DEVICENAME\n</code></pre> </li> <li> <p>Create a local mount point &amp; mount to replay logfile</p> <pre><code>sudo mkdir -vp /mnt/data-0 &amp;&amp; sudo mount /dev/${DEVICENAME} /mnt/data-0/\n</code></pre> </li> <li> <p>Unmount the device</p> <pre><code>sudo umount /mnt/data-0/\n</code></pre> </li> <li> <p>Run check / ncheck</p> <pre><code>sudo xfs_repair -n /dev/${DEVICENAME}; sudo xfs_ncheck /dev/${DEVICENAME}\necho \"If filesystem corruption was corrected due to replay of the logfile, the xfs_ncheck should produce a list of nodes and pathnames, instead of the errorlog.\"\n</code></pre> </li> <li> <p>If needed run xfs repair</p> <pre><code>sudo xfs_repair /dev/${DEVICENAME}\n</code></pre> </li> <li> <p>Logout from target</p> <pre><code>sudo iscsiadm --mode node --targetname iqn.2005-10.org.freenas.ctl:${DISKNAME} --portal storage-server-lagg.lan.stamx.nl --logout\necho \"Volumes are now ready to be mounted as PVCs.\"\n</code></pre> </li> </ol>"},{"location":"cheatsheet/#rsync","title":"Rsync","text":"<p>Run a Rsync exact copy</p> <pre><code>sudo rsync -axHAWXS --numeric-ids --info=progress2 /mnt/sourcePart/ /mnt/destPart\n</code></pre>"},{"location":"cheatsheet/#bitnami-sealed-secret","title":"Bitnami Sealed Secret","text":"<p>Raw mode</p> <pre><code>echo -n foo | kubeseal --cert \"./sealed-secret-tls-2.crt\" --raw --scope cluster-wide\n</code></pre> <p>Create TLS (unencrypted) secret</p> <pre><code>kubectl create secret tls cloudflare-tls --key origin-ca.pk --cert origin-ca.crt --dry-run=client -o yaml &gt; cloudflare-tls.yaml\n</code></pre> <p>Encrypt secret with custom public certificate.</p> <pre><code>kubeseal --cert \"./sealed-secret-tls-2.crt\" --format=yaml &lt; secret.yaml &gt; sealed-secret.yaml\n</code></pre> <p>Add sealed secret to configfile secret</p> <pre><code>echo -n &lt;mypassword_value&gt; | kubectl create secret generic &lt;secretname&gt; --dry-run=client --from-file=&lt;password_key&gt;=/dev/stdin -o json | kubeseal --cert ./sealed-secret-tls-2.crt -o yaml \\\n-n democratic-csi --merge-into &lt;secret&gt;.yaml\n</code></pre> <p>Raw sealed secret</p> <p><code>strict</code> scope (default):</p> <pre><code>echo -n foo | kubeseal --raw --from-file=/dev/stdin --namespace bar --name mysecret\nAgBChHUWLMx...\n</code></pre> <p><code>namespace-wide</code> scope:</p> <pre><code>echo -n foo | kubeseal --cert ./sealed-secret-tls-2.crt --raw --from-file=/dev/stdin --namespace bar --scope namespace-wide\nAgAbbFNkM54...\n</code></pre> <p><code>cluster-wide</code> scope:</p> <pre><code>echo -n foo | kubeseal --cert ./sealed-secret-tls-2.crt --raw --from-file=/dev/stdin --scope cluster-wide\nAgAjLKpIYV+...\n</code></pre> <p>Include the <code>sealedsecrets.bitnami.com/namespace-wide</code> annotation in the <code>SealedSecret</code></p> <pre><code>metadata:\n  annotations:\n    sealedsecrets.bitnami.com/namespace-wide: \"true\"\n</code></pre> <p>Include the <code>sealedsecrets.bitnami.com/cluster-wide</code> annotation in the <code>SealedSecret</code></p> <pre><code>metadata:\n  annotations:\n    sealedsecrets.bitnami.com/cluster-wide: \"true\"\n</code></pre> <p>Github</p> <p>AWS Bitnami tutorial</p> <p>Blogpost Tutorial</p>"},{"location":"cheatsheet/#kubernetes","title":"Kubernetes","text":"<p>Drain and terminate all pods gracefully on the node while marking the node as unschedulable</p> <pre><code>kubectl drain --ignore-daemonsets --delete-emptydir-data &lt;nodename&gt;\n</code></pre> <p>Make the node unschedulable</p> <pre><code>kubectl cordon &lt;nodename&gt;\n</code></pre> <p>Make the node schedulable</p> <pre><code>kubectl uncordon &lt;nodename&gt;\n</code></pre> <p>Convert to BASE64</p> <pre><code>echo -n '&lt;value&gt;' | base64\n</code></pre> <p>Decode a secret with config file data</p> <pre><code>kubectl get secret &lt;secret_name&gt; -o jsonpath='{.data}' -n &lt;namespace&gt;\n</code></pre> <p>Create secret from file</p> <pre><code>kubectl create secret generic &lt;secret name&gt; --from-file=&lt;secret filelocation&gt; --dry-run=client  --output=yaml &gt; secrets.yaml\n</code></pre> <p>Restart Pod</p> <pre><code>kubectl rollout restart deployment &lt;deployment name&gt; -n &lt;namespace&gt;\n</code></pre> <p>Change PV reclaim policy</p> <pre><code>kubectl patch pv &lt;pv-name&gt; -p \"{\\\"spec\\\":{\\\"persistentVolumeReclaimPolicy\\\":\\\"Retain\\\"}}\"\n</code></pre> <p>Shell into pod</p> <pre><code>kubectl exec -it &lt;pod_name&gt; -- /bin/bash\n</code></pre> <p>Copy to or from pod</p> <pre><code>kubectl cp &lt;namespace&gt;/&lt;pod&gt;:/tmp/foo /tmp/bar\n</code></pre>"},{"location":"cheatsheet/#argocd-webui","title":"ArgoCD WebUI","text":"<pre><code>argocd admin dashboard -n argocd\n</code></pre>"},{"location":"cheatsheet/#uptime-kuma","title":"Uptime Kuma","text":"<p>A sqlite query to find and replace a part in the monitor url.</p> <pre><code>UPDATE monitor SET url = REPLACE(url, 'old', 'new') WHERE url LIKE '%old%';\n</code></pre>"},{"location":"desktop/","title":"Desktop","text":""},{"location":"desktop/#install-debian","title":"Install Debian","text":"<p>Install Debian with the netinstall iso image from https://www.debian.org/ without any desktop environment.</p>"},{"location":"desktop/#installation","title":"Installation","text":"<ol> <li> <p>After installation, login and switch to root using su     <pre><code>su\n</code></pre></p> </li> <li> <p>Update apt     <pre><code>apt update\n</code></pre></p> </li> <li> <p>Install desktop and packages     <pre><code>apt install \\\n    gnome-core \\\n    git \\\n    ssh\n</code></pre></p> </li> <li> <p>Reboot     <pre><code>shutdown -r now\n</code></pre></p> </li> </ol>"},{"location":"desktop/#post-install","title":"Post install","text":""},{"location":"desktop/#fix-network","title":"Fix network","text":"<ol> <li> <p>After the restart, login, open the terminal.     <pre><code>su\n</code></pre></p> <pre><code>mv /etc/network/interfaces /etc/network/interfaces.bak\n</code></pre> <p>After the restart, the wired or wireless network should work.</p> <p>Change wifi powersave setting from 3 to 2 in <code>etc/NetworkManager/conf.d/default-wifi-powersave-on.conf</code> to fix wifi issue</p> <pre><code>[connection]\nwifi.powersave = 2\n</code></pre> </li> </ol>"},{"location":"desktop/#sudo","title":"Sudo","text":"<ol> <li> <p>Install sudo     <pre><code>su\n</code></pre></p> <p><pre><code>apt update &amp;&amp; apt install sudo\n</code></pre> 2. Add user to sudoers group <pre><code>sudo visudo\n</code></pre> 3. Add the following line at the end of the file to grant coen user sudo privileges: <pre><code>coen  ALL=(ALL) NOPASSWD: ALL\n</code></pre> This will allow coen to run any command as root without a password prompt (remove NOPASSWD if you want the password prompt to appear).</p> </li> </ol>"},{"location":"desktop/#ansible-playbook","title":"Ansible playbook","text":"<p>Run the Ansible playbook setup the desktop desired state, see Infrastructure as Code page.</p>"},{"location":"hypervisor/","title":"Hypervisor","text":""},{"location":"hypervisor/#install-proxmox","title":"Install Proxmox","text":"<p>Enter the following values during setup.</p> <ol> <li>Install Proxmox with default Partitioning.</li> <li>Hostname <code>pve-b</code> or <code>pve-a</code></li> <li>Domain <code>lan.stamx.nl</code></li> </ol>"},{"location":"hypervisor/#disks-and-partitioning","title":"Disks and partitioning","text":"<ol> <li>Remove local-lvm via the proxmox UI.</li> <li>Remove the data partition.</li> </ol> <pre><code>lvremove pve/data\n</code></pre> <ol> <li>Extend the root logical volume and resize.</li> </ol> <pre><code>lvextend -l +100%FREE /dev/pve/root\n</code></pre> <pre><code>resize2fs /dev/pve/root\n</code></pre> <ol> <li>Update the system.</li> </ol> <pre><code>update-initramfs -u\n</code></pre>"},{"location":"hypervisor/#network","title":"Network","text":"<ol> <li> <p>Backup default network interfaces file</p> <pre><code>cp /etc/network/interfaces /etc/network/interfaces.bak\n</code></pre> </li> <li> <p>Change network settings</p> <pre><code>nano /etc/network/interfaces\n</code></pre> </li> <li> <p>Paste the following content into the interfaces file for each specific     Proxmox hypervisor</p> pve-apve-b <pre><code>auto lo\niface lo inet loopback\n\nauto nic0\niface nic0 inet manual\n    up /sbin/ethtool -s nic0 wol g\n#Onboard\n\niface nic1 inet manual\n#PCI\n\niface nic2 inet manual\n#PCI\n\nauto vmbr0\niface vmbr0 inet static\n        address 10.0.99.2/24\n        gateway 10.0.99.1\n        bridge-ports nic0\n        bridge-stp off\n        bridge-fd 0\n#mgmt\n\nauto vmbr1\niface vmbr1 inet manual\n        bridge-ports nic1\n        bridge-stp off\n        bridge-fd 0\n        bridge-vlan-aware yes\n        bridge-vids 2-4094\n#wan\n\nauto vmbr2\niface vmbr2 inet manual\n        bridge-ports nic2\n        bridge-stp off\n        bridge-fd 0\n        bridge-vlan-aware yes\n        bridge-vids 2-4094\n#lan\n\n\nsource /etc/network/interfaces.d/*\n</code></pre> <pre><code>auto lo\niface lo inet loopback\n\nauto nic0\niface nic0 inet manual\n    up /sbin/ethtool -s nic0 wol g\n#Onboard\n\niface nic1 inet manual\n#PCI\n\niface nic2 inet manual\n#PCI\n\nauto vmbr0\niface vmbr0 inet static\n        address 10.0.99.3/24\n        gateway 10.0.99.1\n        bridge-ports nic0\n        bridge-stp off\n        bridge-fd 0\n#mgmt\n\nauto vmbr1\niface vmbr1 inet manual\n        bridge-ports nic1\n        bridge-stp off\n        bridge-fd 0\n        bridge-vlan-aware yes\n        bridge-vids 2-4094\n#wan\n\nauto vmbr2\niface vmbr2 inet manual\n        bridge-ports nic2\n        bridge-stp off\n        bridge-fd 0\n        bridge-vlan-aware yes\n        bridge-vids 2-4094\n#lan\n\nsource /etc/network/interfaces.d/*\n</code></pre> </li> <li> <p>Reboot</p> <pre><code>reboot now\n</code></pre> </li> </ol>"},{"location":"hypervisor/#import-zfs-pool","title":"Import ZFS pool","text":"<ol> <li> <p>List zfs pools and check if vm-storage pool exist.</p> <pre><code>zpool import\n</code></pre> </li> <li> <p>Import zfs pool</p> <pre><code>zpool import -f vm-storage\n</code></pre> </li> <li> <p>Reboot</p> <pre><code>reboot now\n</code></pre> </li> <li> <p>Add pool to the UI.     Go to <code>Datacenter</code> -&gt; <code>Storage</code> -&gt; <code>Add</code> -&gt; <code>ZFS</code> -&gt; Select <code>vm-storage</code> and     enter id <code>vm-storage</code></p> </li> </ol>"},{"location":"hypervisor/#post-install-scripts","title":"Post install scripts","text":"<p>The scripts can be found at https://community-scripts.github.io/ProxmoxVE/</p> <ul> <li>Run the 'Proxmox VE CPU Scaling Governor' script and set it to <code>powersaving</code> https://community-scripts.github.io/ProxmoxVE/scripts?id=scaling-governor</li> <li>Run the 'Proxmox VE Post Install' script https://community-scripts.github.io/ProxmoxVE/scripts?id=post-pve-install</li> <li>Run the 'Proxmox VE Kernel Clean' script to clean up old kernels https://community-scripts.github.io/ProxmoxVE/scripts?id=kernel-clean</li> </ul>"},{"location":"hypervisor/#high-availability","title":"High Availability","text":""},{"location":"hypervisor/#add-qdevice","title":"Add QDevice","text":""},{"location":"hypervisor/#on-the-qdevice","title":"On the Qdevice","text":"<ol> <li> <p>This command sets the root password, enables root login in the SSH     configuration, and restarts the SSH service.</p> <pre><code>passwd root &amp;&amp; sed -i 's/#PermitRootLogin/PermitRootLogin yes/' /etc/ssh/sshd_config &amp;&amp; systemctl restart sshd\n</code></pre> </li> <li> <p>Install corosync packages</p> <pre><code>apt install corosync-qnetd corosync-qdevice\n</code></pre> </li> </ol>"},{"location":"hypervisor/#on-the-proxmox-nodes","title":"On the Proxmox node(s)","text":"<ol> <li> <p>Install corosync-qdevice packages. Repeat for each node in your cluster.</p> <pre><code>apt update &amp;&amp; apt install corosync-qdevice -y\n</code></pre> </li> <li> <p>Check current status</p> <pre><code>pvecm status\n</code></pre> <p>the output should look like this</p> <pre><code>Cluster information\n-------------------\nName:             pve-cluster\nConfig Version:   2\nTransport:        knet\nSecure auth:      on\n\nQuorum information\n------------------\nDate:             Wed Mar 26 13:41:51 2025\nQuorum provider:  corosync_votequorum\nNodes:            2\nNode ID:          0x00000002\nRing ID:          1.16\nQuorate:          Yes\n\nVotequorum information\n----------------------\nExpected votes:   2\nHighest expected: 2\nTotal votes:      2\nQuorum:           2\nFlags:            Quorate\n\nMembership information\n----------------------\n    Nodeid      Votes Name\n0x00000001          1 10.0.99.2\n0x00000002          1 10.0.99.3 (local)\n</code></pre> </li> <li> <p>Add the QDevice to the cluster. Run this on one of the Proxmox nodes. Change     the IP address to the IP of the Qdevice</p> <pre><code>pvecm qdevice setup 10.0.99.102 -f\n</code></pre> </li> <li> <p>Once this is completed check if the Qdevice has been added to the cluster.</p> <pre><code>pvecm status\n</code></pre> <p>The output should now look similar like this:</p> <pre><code>Cluster information\n-------------------\nName:             pve-cluster\nConfig Version:   3\nTransport:        knet\nSecure auth:      on\n\nQuorum information\n------------------\nDate:             Wed Mar 26 13:51:33 2025\nQuorum provider:  corosync_votequorum\nNodes:            2\nNode ID:          0x00000002\nRing ID:          1.16\nQuorate:          Yes\n\nVotequorum information\n----------------------\nExpected votes:   3\nHighest expected: 3\nTotal votes:      3\nQuorum:           2\nFlags:            Quorate Qdevice\n\nMembership information\n----------------------\n    Nodeid      Votes    Qdevice Name\n0x00000001          1    A,V,NMW 10.0.99.2\n0x00000002          1    A,V,NMW 10.0.99.3 (local)\n0x00000000          1            Qdevice\n</code></pre> </li> </ol>"},{"location":"hypervisor/#make-a-vm-ha","title":"Make a VM HA","text":"<ol> <li>Turn on Replication for the VM that you want to make High Available.</li> <li>Go to <code>Datacenter</code> -&gt; <code>HA</code> and add a Resource.</li> </ol>"},{"location":"hypervisor/#mount-usb-stick-backups","title":"Mount USB stick (backups)","text":"<ol> <li> <p>Find the USB stick device     <pre><code>lsblk\n</code></pre></p> </li> <li> <p>Create mount directory     <pre><code>mkdir -p /mnt/pve/backup_usb-stick\n</code></pre></p> </li> <li> <p>Mount (change <code>&lt;sd?&gt;</code> to the correct device)     <pre><code>mount /dev/&lt;sd?&gt; /mnt/pve/backup_usb-stick\n</code></pre></p> </li> </ol>"},{"location":"infrastructure-as-code/","title":"Infrastructure as Code","text":""},{"location":"infrastructure-as-code/#ansible","title":"Ansible","text":""},{"location":"infrastructure-as-code/#installation","title":"Installation","text":"<ol> <li> <p>Remove old Debian package</p> <p><pre><code>sudo apt remove ansible -y\n</code></pre> 2. Install pipx</p> <pre><code>sudo apt update\nsudo apt install pipx\npipx ensurepath\n</code></pre> </li> <li> <p>Install the minimal <code>ansible-core</code> package using pipx</p> <pre><code>pipx install ansible-core\n</code></pre> </li> <li> <p>Clone the infrastructure repository    <pre><code> git clone https://github.com/x-real-ip/infrastructure.git\n</code></pre></p> </li> <li> <p>Navigate to the <code>ansible</code> directory</p> </li> <li> <p>Install collections from the requirements.yaml located in the Ansible dir (optional)</p> <pre><code>ansible-galaxy collection install -r requirements.yaml\n</code></pre> <p>requirements.yaml example</p> <pre><code>collections:\n  - name: ansible.posix\n  - name: kubernetes.core\n  - name: community.general\n</code></pre> </li> </ol>"},{"location":"infrastructure-as-code/#playbooks","title":"Playbooks","text":"<p>Navigate to the <code>ansible</code> directory where the infrastructure repository is cloned.</p> Playbook Command Comment desktop <code>ansible-playbook -K --ask-vault-password playbooks/desktop.yaml</code> Set the Debian desktop desired state k3s_apply-apps-with-truenas-storage <code>ansible-playbook --ask-vault-password playbooks/k3s_apply-apps-with-truenas-storage.yaml</code> Apply all k8s resources that has storage=truenas label k3s_install_cluster_bare <code>ansible-playbook --ask-vault-password playbooks/k3s_install_cluster_bare.yaml</code> Install or update k3s on all nodes without installing additional deployments k3s_install_cluster_minimal <code>ansible-playbook --ask-vault-password playbooks/k3s_install_cluster_minimal.yaml</code> Install or update k3s on all nodes including additional deployments k3s_remove-apps-with-truenas-storage <code>ansible-playbook --ask-vault-password playbooks/k3s_remove-apps-with-truenas-storage.yaml</code> Delete all k8s resources that has storage=truenas label k3s_render_yaml <code>ansible-playbook --ask-vault-password playbooks/k3s_render_yaml.yaml</code> Ouput all kubernetes k8s yamls from jinja2 k3s_rolling-update-nodes <code>ansible-playbook --ask-vault-password playbooks/k3s_rolling-update-nodes.yaml</code> Update the os packages on all k3s nodes k3s_start_all_pods <code>ansible-playbook playbooks/k3s_start_all_pods.yaml</code> Uncordon nodes k3s_stop_all_pods <code>ansible-playbook playbooks/k3s_stop_all_pods.yaml</code> Cordon and drain nodes known_hosts <code>ansible-playbook playbooks/known_hosts.yaml</code> Add devices to the local known_hosts file local_self-signed-certificate_generate <code>ansible-playbook playbooks/local_self-signed-certificate_generate.yaml</code> Generate a self-signed certificate non-root-user <code>ansible-playbook -k --ask-vault-password playbooks/non-root-user.yaml</code> Add a non root user proxmox_migrate-vms-to-master <code>ansible-playbook playbooks/proxmox_migrate-vms-to-master.yaml</code> Migrate all possible VM's to the Proxmox node that is marked as master proxmox_wake-up-standby-node <code>ansible-playbook playbooks/proxmox_wake-up-standby-node.yaml</code> Send a magic packet to wakeup the standby (not master) Proxmox node shelly_update-firmware <code>ansible-playbook --ask-vault-password playbooks/shelly_update-firmware.yaml</code> Toggle allow internet firewall rule for NOT VLAN, update and set desired state of all Shelly devices truenas_shares <code>ansible-playbook playbooks/truenas_shares.yaml</code> Configure all NFS and ISCSI shares on the truenas hosts truenas_switch-master <code>ansible-playbook --ask-vault-password playbooks/truenas_switch-master.yaml</code> Switch the master from A to B or the otherway around truenas_snapshot-tasks <code>ansible-playbook --ask-vault-password playbooks/truenas_snapshot-tasks.yaml</code> Apply desired snapshots tasks to the truenas server"},{"location":"kubernetes/","title":"Kubernetes","text":""},{"location":"kubernetes/#prerequisites","title":"Prerequisites","text":"<ul> <li>Ansible installed on your local machine. See Infrastructure as Code page.</li> <li>SSH access to the target machines where you want to install k3s.</li> </ul>"},{"location":"kubernetes/#add-ssh-keys-on-local-device","title":"Add SSH keys on local device","text":"<ol> <li> <p>Copy private and public key (stored in Bitwarden Vault)</p> <pre><code>cp /path/to/my/key/ansible ~/.ssh/ansible\ncp /path/to/my/key/ansible.pub ~/.ssh/ansible.pub\n</code></pre> </li> <li> <p>Change permissions on the files</p> <pre><code>sudo chmod 600 ~/.ssh/ansible\nsudo chmod 600 ~/.ssh/ansible.pub\n</code></pre> </li> <li> <p>Make ssh agent to actually use copied key</p> <pre><code>ssh-add ~/.ssh/ansible\n</code></pre> </li> </ol>"},{"location":"kubernetes/#setting-up-vm-hosts-on-proxmox","title":"Setting up VM hosts on Proxmox","text":"<ol> <li>Create a VM with the following partitions<ul> <li>/ (10 GB) xfs</li> <li>/var (50GB) xfs</li> <li>/boot (1GB)</li> </ul> </li> </ol> <p>Note</p> <p>No swap partition is needed for kubernetes</p> <ol> <li>Login to the VM and set hostname.    <pre><code>hostnamectl set-hostname &lt;hostname&gt;\n</code></pre></li> <li>Reboot.</li> <li> <p>Set static ip for the VM in the DHCP server.</p> </li> <li> <p>Reboot.</p> </li> <li>Set this hostname in the ansible inventory <code>hosts.yaml</code> file.</li> </ol>"},{"location":"network/","title":"Network","text":""},{"location":"network/#vlans","title":"VLAN's","text":"<ul> <li>Use VLAN 6 for KPN</li> <li>USE VLAN 300 for Odido</li> </ul> VLAN Description 6 wan 10 private 20 guest 30 iot 40 not 50 kids 99 management 100 systems"},{"location":"network/#switch-ports-and-vlan-assignments","title":"Switch Ports and VLAN Assignments","text":"Device Patch Port Switch Port VLAN Untagged VLAN Tagged PVID WAN NTU 4 1 6 6 wallpatch-basement 10 2 99 99 wallpatch-livingroom 8 3 99 10, 20, 30, 40, 50 99 tado - 4 30 30 k3s-wkr-01 16 5 100 100 ap-outdoor 6 6 99 10, 20, 30, 40, 50 99 ap-upstairs 7 7 99 10, 20, 30, 40, 50 99 ap-downstairs 18 8 99 10, 20, 30, 40, 50 99 shield 9 9 30 30 10 100 100 k3s-wkr-02 17 11 99 99 tv-01 12 12 30 30 wallpatch-livingroom 13 13 100 100 inverter 14 14 30 30 pve-a - WAN-A - 15 6 6 pve-b - WAN-B - 16 6 6 opnsense pve-a - LAN-A - 17 99 10, 20, 30, 40, 50, 99, 100 99 opnsense pve-b - LAN-B - 18 99 10, 20, 30, 40, 50, 99, 100 99 pve-a - management (C) - 19 99 99 pve-b - management (D) - 20 99 99 19 21 99 99 22 99 99 23 100 100 24 100 100 25 99 99 26 99 99 27 99 99 28 99 99"},{"location":"network/#devices-in-management-vlan-99","title":"Devices in management VLAN 99","text":"Device VLAN MAC IP firewall / router 99 10.0.99.1 pve-a 99 10.0.99.2 pve-b 99 10.0.99.3 switch 99 10.0.99.4 ap-livingroom 99 A8:29:48:6E:4B:B8 dhcp ap-upstairs 99 b0:4e:26:12:35:fa dhcp ap-downstairs 99 b0:4e:26:86:08:aa dhcp ap-outdoor 99 68:ff:7b:0a:40:1a dhcp"},{"location":"odroid/","title":"Odroid","text":""},{"location":"odroid/#installation","title":"Installation","text":"<ol> <li>Download the minimal .xz-compressed image file from https://fi.mirror.armbian.de/archive/odroidc4/archive/</li> <li>Write the .xz compressed image with balenaEtcher</li> <li>Insert the SD/MMC and boot</li> <li>Login via SSH user <code>root</code> default password <code>1234</code></li> </ol>"}]}